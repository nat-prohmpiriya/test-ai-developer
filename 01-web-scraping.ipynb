{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping: Thailand Yellow Pages\n",
    "\n",
    "## Objective\n",
    "ดึงข้อมูลรายชื่อธุรกิจประเภท **คลินิก** จากเว็บไซต์ Thailand Yellow Pages (https://www.yellowpages.co.th/) และจัดเก็บใน Pandas DataFrame\n",
    "\n",
    "## Data to Extract\n",
    "- ชื่อธุรกิจ (Business Name)\n",
    "- ที่อยู่/จังหวัด (Location/Province)\n",
    "- เบอร์โทรศัพท์ (Phone Number)\n",
    "- รายละเอียดบริการ (Description)\n",
    "- เว็บไซต์ (Website - if available)\n",
    "\n",
    "## Approach\n",
    "1. ใช้ `requests` เพื่อดึง HTML จากเว็บไซต์\n",
    "2. ใช้ `BeautifulSoup` เพื่อ parse HTML และดึงข้อมูล\n",
    "3. จัดเก็บข้อมูลใน Pandas DataFrame\n",
    "4. Export เป็น CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install requests beautifulsoup4 pandas lxml\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from typing import List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "กำหนดค่า URL และ Headers สำหรับการ request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for clinic listings\n",
    "BASE_URL = \"https://www.yellowpages.co.th\"\n",
    "CATEGORY_URL = f\"{BASE_URL}/heading/คลินิก\"\n",
    "\n",
    "# Headers to mimic browser request\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'th-TH,th;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "    'Connection': 'keep-alive',\n",
    "}\n",
    "\n",
    "# Number of pages to scrape (limit for demo)\n",
    "MAX_PAGES = 5\n",
    "\n",
    "print(f\"Target URL: {CATEGORY_URL}\")\n",
    "print(f\"Pages to scrape: {MAX_PAGES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "สร้างฟังก์ชันสำหรับการดึงและ parse ข้อมูล"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(url: str, retries: int = 3) -> Optional[BeautifulSoup]:\n",
    "    \"\"\"\n",
    "    Fetch HTML content from URL and return BeautifulSoup object.\n",
    "    \n",
    "    Args:\n",
    "        url: URL to fetch\n",
    "        retries: Number of retry attempts\n",
    "    \n",
    "    Returns:\n",
    "        BeautifulSoup object or None if failed\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.content, 'lxml')\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(2)\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_text(element, default: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Safely extract text from BeautifulSoup element.\n",
    "    \n",
    "    Args:\n",
    "        element: BeautifulSoup element\n",
    "        default: Default value if element is None\n",
    "    \n",
    "    Returns:\n",
    "        Extracted text or default value\n",
    "    \"\"\"\n",
    "    if element:\n",
    "        return element.get_text(strip=True)\n",
    "    return default\n",
    "\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Page Structure\n",
    "\n",
    "ดึงหน้าแรกมาวิเคราะห์โครงสร้าง HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the first page to analyze structure\n",
    "soup = fetch_page(CATEGORY_URL)\n",
    "\n",
    "if soup:\n",
    "    print(\"Page fetched successfully!\")\n",
    "    print(f\"Page title: {soup.title.string if soup.title else 'N/A'}\")\n",
    "else:\n",
    "    print(\"Failed to fetch page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze page structure - find business listing containers\nif soup:\n    # Use the correct selector for Yellow Pages\n    listings = soup.select('div.yp-search-listing')\n    print(f\"Found {len(listings)} business listings on the page\")\n    \n    if listings:\n        # Show first listing structure\n        first_listing = listings[0]\n        print(\"\\n--- First listing preview ---\")\n        \n        # Extract sample data\n        name = first_listing.select_one('.yp-listing-title h3 a')\n        address = first_listing.select_one('p.yp-listing-address')\n        desc = first_listing.select_one('p.yp-listing-desc')\n        \n        if name:\n            print(f\"Name: {name.get_text(strip=True)}\")\n        if address:\n            print(f\"Address: {address.get_text(strip=True)}\")\n        if desc:\n            print(f\"Description: {desc.get_text(strip=True)[:100]}...\")\nelse:\n    print(\"Failed to fetch page - check connection\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scraping Function\n",
    "\n",
    "ฟังก์ชันหลักสำหรับดึงข้อมูลธุรกิจจากแต่ละหน้า"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def parse_business_listing(card) -> Dict:\n    \"\"\"\n    Parse a single business listing card and extract information.\n\n    Structure of Yellow Pages listing (based on actual HTML analysis):\n    - Container: div.yp-search-listing\n    - Name: h3 a (inside yp-listing-title)\n    - Website: a.yp-listing-website\n    - Address: p.yp-listing-address\n    - Description: p.yp-listing-desc\n    - Category: listing-category-section a\n\n    Args:\n        card: BeautifulSoup element containing business info\n\n    Returns:\n        Dictionary with business information\n    \"\"\"\n    business = {\n        'name': '',\n        'address': '',\n        'description': '',\n        'website': '',\n        'category': '',\n        'profile_url': ''\n    }\n\n    try:\n        # Extract business name from h3 > a inside yp-listing-title\n        name_elem = card.select_one('.yp-listing-title h3 a')\n        if name_elem:\n            business['name'] = extract_text(name_elem)\n            business['profile_url'] = name_elem.get('href', '')\n\n        # Extract address from p.yp-listing-address\n        address_elem = card.select_one('p.yp-listing-address')\n        if address_elem:\n            business['address'] = extract_text(address_elem)\n\n        # Extract description from p.yp-listing-desc\n        desc_elem = card.select_one('p.yp-listing-desc')\n        if desc_elem:\n            # Clean description and limit length\n            desc_text = extract_text(desc_elem)\n            business['description'] = desc_text[:300] if len(desc_text) > 300 else desc_text\n\n        # Extract website from a.yp-listing-website\n        website_elem = card.select_one('a.yp-listing-website')\n        if website_elem:\n            business['website'] = website_elem.get('href', '')\n\n        # Extract category from listing-category-section\n        category_elem = card.select_one('.listing-category-section a')\n        if category_elem:\n            business['category'] = extract_text(category_elem)\n\n    except Exception as e:\n        print(f\"Error parsing listing: {e}\")\n\n    return business\n\n\ndef scrape_page(url: str) -> List[Dict]:\n    \"\"\"\n    Scrape all business listings from a single page.\n\n    Args:\n        url: Page URL to scrape\n\n    Returns:\n        List of business dictionaries\n    \"\"\"\n    businesses = []\n    soup = fetch_page(url)\n\n    if not soup:\n        return businesses\n\n    # Find all business listing cards using the correct selector\n    # Each listing is in: div.yp-search-listing\n    cards = soup.select('div.yp-search-listing')\n\n    print(f\"  Found {len(cards)} listing cards on page\")\n\n    for card in cards:\n        business = parse_business_listing(card)\n        if business['name']:  # Only add if name was found\n            businesses.append(business)\n\n    return businesses\n\n\nprint(\"Scraping functions defined!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Execute Scraping\n",
    "\n",
    "ดึงข้อมูลจากหลายหน้า"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_pages(base_url: str, max_pages: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scrape multiple pages of business listings.\n",
    "    \n",
    "    Args:\n",
    "        base_url: Base category URL\n",
    "        max_pages: Maximum number of pages to scrape\n",
    "    \n",
    "    Returns:\n",
    "        List of all business dictionaries\n",
    "    \"\"\"\n",
    "    all_businesses = []\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        if page == 1:\n",
    "            url = base_url\n",
    "        else:\n",
    "            url = f\"{base_url}/page/{page}\"\n",
    "        \n",
    "        print(f\"Scraping page {page}/{max_pages}: {url}\")\n",
    "        \n",
    "        businesses = scrape_page(url)\n",
    "        all_businesses.extend(businesses)\n",
    "        \n",
    "        print(f\"  Found {len(businesses)} businesses (Total: {len(all_businesses)})\")\n",
    "        \n",
    "        # Polite delay between requests\n",
    "        if page < max_pages:\n",
    "            delay = random.uniform(1, 3)\n",
    "            print(f\"  Waiting {delay:.1f}s...\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    return all_businesses\n",
    "\n",
    "\n",
    "# Execute scraping\n",
    "print(\"Starting scraping...\\n\")\n",
    "all_businesses = scrape_all_pages(CATEGORY_URL, MAX_PAGES)\n",
    "print(f\"\\nScraping complete! Total businesses collected: {len(all_businesses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create DataFrame\n",
    "\n",
    "แปลงข้อมูลที่ดึงมาเป็น Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from scraped data\n",
    "df = pd.DataFrame(all_businesses)\n",
    "\n",
    "# Display basic info\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Data summary statistics\nprint(\"=\" * 50)\nprint(\"DATA SUMMARY\")\nprint(\"=\" * 50)\nprint(f\"\\nTotal records: {len(df)}\")\nprint(f\"\\nNon-empty value counts:\")\nfor col in df.columns:\n    non_empty = (df[col] != '').sum()\n    pct = (non_empty/len(df)*100) if len(df) > 0 else 0\n    print(f\"  {col}: {non_empty} ({pct:.1f}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Cleaning\n",
    "\n",
    "ทำความสะอาดข้อมูลที่ดึงมา"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clean the data\ndf_cleaned = df.copy()\n\n# Remove duplicates based on name\ninitial_count = len(df_cleaned)\ndf_cleaned = df_cleaned.drop_duplicates(subset=['name'], keep='first')\nprint(f\"Removed {initial_count - len(df_cleaned)} duplicate entries\")\n\n# Remove rows with empty names\ndf_cleaned = df_cleaned[df_cleaned['name'] != '']\nprint(f\"Records after cleaning: {len(df_cleaned)}\")\n\n# Reset index\ndf_cleaned = df_cleaned.reset_index(drop=True)\n\n# Reorder columns for better readability\ncolumn_order = ['name', 'category', 'address', 'description', 'website', 'profile_url']\ndf_cleaned = df_cleaned[column_order]\n\ndf_cleaned.head(10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export to CSV\n",
    "\n",
    "บันทึกข้อมูลเป็นไฟล์ CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "output_file = 'clinic_listings_yellowpages.csv'\n",
    "df_cleaned.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Data exported to: {output_file}\")\n",
    "print(f\"Total records: {len(df_cleaned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "สรุปผลการดึงข้อมูล"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"WEB SCRAPING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nSource: Thailand Yellow Pages (yellowpages.co.th)\")\n",
    "print(f\"Category: คลินิก (Clinic)\")\n",
    "print(f\"Pages scraped: {MAX_PAGES}\")\n",
    "print(f\"Total records collected: {len(df_cleaned)}\")\n",
    "print(f\"Output file: {output_file}\")\n",
    "print(\"\\nColumns in dataset:\")\n",
    "for col in df_cleaned.columns:\n",
    "    print(f\"  - {col}\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final display of data\n",
    "df_cleaned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}