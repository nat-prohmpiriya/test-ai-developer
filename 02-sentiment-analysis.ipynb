{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis: Thai Text Classification\n",
    "\n",
    "## Objective\n",
    "‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å (Sentiment Analysis) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡πÄ‡∏õ‡πá‡∏ô **positive**, **neutral**, ‡∏´‡∏£‡∏∑‡∏≠ **negative**\n",
    "\n",
    "## Approach\n",
    "‡πÉ‡∏ä‡πâ **Pre-trained Model** ‡∏à‡∏≤‡∏Å Hugging Face ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å fine-tuned ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡πâ‡∏ß:\n",
    "- **Model**: WangchanBERTa (fine-tuned for sentiment analysis)\n",
    "- **Dataset**: Wisesight Sentiment Corpus (Thai social media text)\n",
    "\n",
    "## Evaluation Metrics\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-score\n",
    "- Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install transformers datasets torch scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Dict, Tuple\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Hugging Face libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# Scikit-learn for evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "‡πÉ‡∏ä‡πâ **Wisesight Sentiment Corpus** - ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏à‡∏≤‡∏Å Social Media ‡∏û‡∏£‡πâ‡∏≠‡∏° labels:\n",
    "- `pos` (positive)\n",
    "- `neu` (neutral)\n",
    "- `neg` (negative)\n",
    "- `q` (question) - ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡∏≠‡∏≠‡∏Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wisesight Sentiment dataset from Hugging Face\n",
    "print(\"Loading Wisesight Sentiment dataset...\")\n",
    "dataset = load_dataset(\"wisesight_sentiment\")\n",
    "\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(dataset)\n",
    "\n",
    "print(f\"\\nTrain set size: {len(dataset['train'])}\")\n",
    "print(f\"Validation set size: {len(dataset['validation'])}\")\n",
    "print(f\"Test set size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset\n",
    "print(\"Sample data from training set:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Label mapping\n",
    "label_names = {0: 'neg', 1: 'neu', 2: 'pos', 3: 'q'}\n",
    "\n",
    "for i in range(5):\n",
    "    sample = dataset['train'][i]\n",
    "    print(f\"\\nText: {sample['texts'][:100]}...\")\n",
    "    print(f\"Label: {label_names[sample['category']]}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check label distribution\n",
    "train_labels = [label_names[x['category']] for x in dataset['train']]\n",
    "test_labels = [label_names[x['category']] for x in dataset['test']]\n",
    "\n",
    "print(\"Label distribution in training set:\")\n",
    "print(pd.Series(train_labels).value_counts())\n",
    "\n",
    "print(\"\\nLabel distribution in test set:\")\n",
    "print(pd.Series(test_labels).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 3 classes: positive, neutral, negative (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_prepare_data(dataset_split):\n",
    "    \"\"\"\n",
    "    Filter out 'question' category and prepare data for evaluation.\n",
    "    \n",
    "    Args:\n",
    "        dataset_split: HuggingFace dataset split\n",
    "    \n",
    "    Returns:\n",
    "        texts: List of text strings\n",
    "        labels: List of label strings (neg, neu, pos)\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    label_map = {0: 'negative', 1: 'neutral', 2: 'positive', 3: 'question'}\n",
    "    \n",
    "    for item in dataset_split:\n",
    "        label = label_map[item['category']]\n",
    "        # Skip question category\n",
    "        if label != 'question':\n",
    "            texts.append(item['texts'])\n",
    "            labels.append(label)\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "# Prepare test data (filter out questions)\n",
    "test_texts, test_labels = filter_and_prepare_data(dataset['test'])\n",
    "\n",
    "print(f\"Test samples after filtering: {len(test_texts)}\")\n",
    "print(f\"\\nLabel distribution after filtering:\")\n",
    "print(pd.Series(test_labels).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Pre-trained Model\n",
    "\n",
    "‡πÉ‡∏ä‡πâ **WangchanBERTa** ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å fine-tuned ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sentiment analysis ‡∏ö‡∏ô Wisesight dataset\n",
    "\n",
    "Model ‡∏ô‡∏µ‡πâ‡∏°‡∏µ accuracy ~91% ‡∏ö‡∏ô Wisesight dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained sentiment model\n",
    "MODEL_NAME = \"poom-sci/WangchanBERTa-finetuned-sentiment\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(\"This may take a minute...\")\n",
    "\n",
    "# Create sentiment analysis pipeline\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=MODEL_NAME,\n",
    "    tokenizer=MODEL_NAME,\n",
    "    max_length=512,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with sample texts\n",
    "sample_texts = [\n",
    "    \"‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏≠‡∏£‡πà‡∏≠‡∏¢‡∏°‡∏≤‡∏Å ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏î‡∏µ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏° ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö\",\n",
    "    \"‡∏£‡πâ‡∏≤‡∏ô‡∏ô‡∏µ‡πâ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤‡∏°‡∏≤‡∏Å ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏û‡∏¥‡πÄ‡∏®‡∏©\",\n",
    "    \"‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å ‡∏£‡∏≠‡∏ô‡∏≤‡∏ô‡∏°‡∏≤‡∏Å ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏≠‡∏£‡πà‡∏≠‡∏¢ ‡πÑ‡∏°‡πà‡∏°‡∏≤‡∏≠‡∏µ‡∏Å‡πÅ‡∏•‡πâ‡∏ß\",\n",
    "    \"‡∏Ç‡∏≠‡∏á‡∏î‡∏µ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ñ‡∏π‡∏Å ‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡πà‡∏≤‡∏°‡∏≤‡∏Å‡πÜ\",\n",
    "    \"‡∏ú‡∏¥‡∏î‡∏´‡∏ß‡∏±‡∏á‡∏°‡∏≤‡∏Å ‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÄ‡∏•‡∏¢\"\n",
    "]\n",
    "\n",
    "print(\"Testing model with sample texts:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in sample_texts:\n",
    "    result = sentiment_analyzer(text)[0]\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Prediction: {result['label']} (confidence: {result['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Predictions on Test Set\n",
    "\n",
    "‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏ö‡∏ô test set ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment_batch(texts: List[str], batch_size: int = 32) -> List[str]:\n",
    "    \"\"\"\n",
    "    Predict sentiment for a list of texts in batches.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "        batch_size: Number of texts to process at once\n",
    "    \n",
    "    Returns:\n",
    "        List of predicted labels\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    total = len(texts)\n",
    "    \n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        results = sentiment_analyzer(batch)\n",
    "        \n",
    "        for result in results:\n",
    "            # Map model output to standard labels\n",
    "            label = result['label'].lower()\n",
    "            if 'pos' in label:\n",
    "                predictions.append('positive')\n",
    "            elif 'neg' in label:\n",
    "                predictions.append('negative')\n",
    "            else:\n",
    "                predictions.append('neutral')\n",
    "        \n",
    "        # Progress update\n",
    "        if (i + batch_size) % 100 == 0 or (i + batch_size) >= total:\n",
    "            print(f\"Progress: {min(i + batch_size, total)}/{total} samples processed\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Run predictions on test set\n",
    "print(\"Running predictions on test set...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "predicted_labels = predict_sentiment_batch(test_texts)\n",
    "\n",
    "print(f\"\\nPrediction complete! Total predictions: {len(predicted_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Metrics\n",
    "\n",
    "‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics ‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û:\n",
    "- **Accuracy**: ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ñ‡∏π‡∏Å\n",
    "- **Precision**: ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ positive\n",
    "- **Recall**: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏ö positive ‡∏à‡∏£‡∏¥‡∏á\n",
    "- **F1-score**: Harmonic mean ‡∏Ç‡∏≠‡∏á precision ‡πÅ‡∏•‡∏∞ recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "precision = precision_score(test_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(test_labels, predicted_labels, average='weighted')\n",
    "f1 = f1_score(test_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nAccuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(test_labels, predicted_labels, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix\n",
    "\n",
    "‡πÅ‡∏™‡∏î‡∏á Confusion Matrix ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "labels_order = ['negative', 'neutral', 'positive']\n",
    "cm = confusion_matrix(test_labels, predicted_labels, labels=labels_order)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=labels_order,\n",
    "    yticklabels=labels_order\n",
    ")\n",
    "plt.title('Confusion Matrix - Sentiment Analysis', fontsize=14)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrix saved to: confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized confusion matrix (percentage)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm_normalized, \n",
    "    annot=True, \n",
    "    fmt='.2%', \n",
    "    cmap='Greens',\n",
    "    xticklabels=labels_order,\n",
    "    yticklabels=labels_order\n",
    ")\n",
    "plt.title('Normalized Confusion Matrix (Percentage)', fontsize=14)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_normalized.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Analysis\n",
    "\n",
    "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà model ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏¥‡∏î"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "errors = []\n",
    "for i, (text, true_label, pred_label) in enumerate(zip(test_texts, test_labels, predicted_labels)):\n",
    "    if true_label != pred_label:\n",
    "        errors.append({\n",
    "            'text': text[:100] + '...' if len(text) > 100 else text,\n",
    "            'true_label': true_label,\n",
    "            'predicted_label': pred_label\n",
    "        })\n",
    "\n",
    "print(f\"Total misclassified samples: {len(errors)} / {len(test_texts)} ({len(errors)/len(test_texts)*100:.2f}%)\")\n",
    "\n",
    "# Show some error examples\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE MISCLASSIFICATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, error in enumerate(errors[:10]):\n",
    "    print(f\"\\n[{i+1}] Text: {error['text']}\")\n",
    "    print(f\"    True: {error['true_label']} | Predicted: {error['predicted_label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Demo\n",
    "\n",
    "‡∏ó‡∏î‡∏™‡∏≠‡∏ö model ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze sentiment of a single text.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with sentiment and confidence\n",
    "    \"\"\"\n",
    "    result = sentiment_analyzer(text)[0]\n",
    "    \n",
    "    # Map to standard labels\n",
    "    label = result['label'].lower()\n",
    "    if 'pos' in label:\n",
    "        sentiment = 'positive'\n",
    "    elif 'neg' in label:\n",
    "        sentiment = 'negative'\n",
    "    else:\n",
    "        sentiment = 'neutral'\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'sentiment': sentiment,\n",
    "        'confidence': result['score']\n",
    "    }\n",
    "\n",
    "\n",
    "# Test with custom texts\n",
    "custom_texts = [\n",
    "    \"‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ‡∏°‡∏≤‡∏Å ‡∏™‡πà‡∏á‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å‡πÜ ‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡πÉ‡∏à‡∏Ñ‡πà‡∏∞\",\n",
    "    \"‡∏õ‡∏Å‡∏ï‡∏¥‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤ ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏ß‡πà‡∏≤‡∏î‡∏µ‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏¢‡πà\",\n",
    "    \"‡∏´‡πà‡∏ß‡∏¢‡πÅ‡∏ï‡∏Å‡∏°‡∏≤‡∏Å ‡πÑ‡∏°‡πà‡∏Ñ‡∏∏‡πâ‡∏°‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏•‡∏¢ ‡∏≠‡∏¢‡πà‡∏≤‡∏ã‡∏∑‡πâ‡∏≠\",\n",
    "    \"‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ô‡πà‡∏≤‡∏£‡∏±‡∏Å‡∏°‡∏≤‡∏Å ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏î‡∏µ\",\n",
    "    \"‡∏£‡∏™‡∏ä‡∏≤‡∏ï‡∏¥‡∏û‡∏≠‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏£‡∏≤‡∏Ñ‡∏≤‡πÅ‡∏û‡∏á‡πÑ‡∏õ\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SENTIMENT ANALYSIS DEMO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in custom_texts:\n",
    "    result = analyze_sentiment(text)\n",
    "    emoji = {'positive': 'üòä', 'neutral': 'üòê', 'negative': 'üòû'}[result['sentiment']]\n",
    "    print(f\"\\n{emoji} [{result['sentiment'].upper()}] (conf: {result['confidence']:.2%})\")\n",
    "    print(f\"   \\\"{text}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SENTIMENT ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Dataset: Wisesight Sentiment Corpus\")\n",
    "print(f\"ü§ñ Model: {MODEL_NAME}\")\n",
    "print(f\"üìù Test samples: {len(test_texts)}\")\n",
    "\n",
    "print(f\"\\nüìà Performance Metrics:\")\n",
    "print(f\"   ‚Ä¢ Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"   ‚Ä¢ Precision: {precision:.4f}\")\n",
    "print(f\"   ‚Ä¢ Recall:    {recall:.4f}\")\n",
    "print(f\"   ‚Ä¢ F1-score:  {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nüìÅ Output files:\")\n",
    "print(f\"   ‚Ä¢ confusion_matrix.png\")\n",
    "print(f\"   ‚Ä¢ confusion_matrix_normalized.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'text': test_texts,\n",
    "    'true_label': test_labels,\n",
    "    'predicted_label': predicted_labels\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('sentiment_predictions.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"Predictions saved to: sentiment_predictions.csv\")\n",
    "\n",
    "# Display sample\n",
    "results_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
